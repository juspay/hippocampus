# ═══════════════════════════════════════════
#  NeuroStore Server — Environment Variables
#  All prefixed NS_ (NeuroStore)
# ═══════════════════════════════════════════

# ── Auth ───────────────────────────────────
# If set, every /api/* request must provide this key
# via X-API-Key header or Authorization: Bearer <key>
# Leave blank to disable auth (local dev)
NS_API_KEY=

# ── Server ─────────────────────────────────
NS_PORT=3000
NS_HOST=0.0.0.0
NS_LOG_LEVEL=info

# ── Database ───────────────────────────────
# Option A: Postgres via connection string
# NS_DATABASE_URL=postgres://user:pass@localhost:5432/neurostore

# Option B: Postgres via individual vars
# NS_PG_HOST=localhost
# NS_PG_PORT=5432
# NS_PG_DATABASE=neurostore
# NS_PG_USER=postgres
# NS_PG_PASSWORD=
# NS_PG_SSL=false

# Option C: SQLite (default if no PG vars set)
# NS_SQLITE_PATH=./data/neurostore.sqlite

# ── Embedding Provider ─────────────────────
# Provider: native | openai | ollama
NS_EMBEDDER_PROVIDER=native

# Model (depends on provider):
#   - openai: text-embedding-3-small (default), text-embedding-3-large, text-embedding-ada-002
#   - ollama: nomic-embed-text (default), mxbai-embed-large, all-minilm
#   - native: ignored (hash-based, no model)
# NS_EMBEDDER_MODEL=nomic-embed-text

# Embedding dimensions:
#   - openai: 1536 (default for small), 3072 (for large)
#   - ollama: 768 (default for nomic), varies by model
#   - native: 384 (default)
# NS_EMBEDDING_DIMENSIONS=768

# ── Completion Provider ────────────────────
# Provider: native | openai | ollama
NS_COMPLETION_PROVIDER=native

# Model (depends on provider):
#   - openai: gpt-4-turbo (default), gpt-4o, gpt-3.5-turbo
#   - ollama: llama3 (default), llama3.2, mistral, phi3
#   - native: ignored (passthrough, no extraction)
# NS_COMPLETION_MODEL=llama3.2

# ── OpenAI (only if using openai provider) ─
# NS_OPENAI_API_KEY=sk-...

# ── Ollama (only if using ollama provider) ─
# NS_OLLAMA_BASE_URL=http://localhost:11434

# ── Custom Prompts ────────────────────────
# Override the default fact extraction prompt
# Must instruct LLM to return JSON with: facts[], strand, temporalFacts[]
# NS_EXTRACTION_PROMPT=Your custom extraction prompt here...
