# ═══════════════════════════════════════════
#  Hippocampus Server — Environment Variables
# ═══════════════════════════════════════════

# ── Auth ───────────────────────────────────
# If set, every /api/* request must provide this key
# via X-API-Key header or Authorization: Bearer <key>
# Leave blank to disable auth (local dev)
HC_API_KEY=

# ── Server ─────────────────────────────────
HC_PORT=4477
HC_HOST=0.0.0.0

# Log levels: comma-separated list of levels to show
# Options: debug, info, warn, error, off (default: off)
# Examples: "info,warn,error" or "error" or "off"
HC_LOG_LEVEL=off

# ── Database ───────────────────────────────
# Option A: Postgres via connection string
# HC_DATABASE_URL=postgres://user:pass@localhost:5432/hippocampus

# Option B: Postgres via individual vars
# HC_PG_HOST=localhost
# HC_PG_PORT=5432
# HC_PG_DATABASE=hippocampus
# HC_PG_USER=postgres
# HC_PG_PASSWORD=
# HC_PG_SSL=false

# Option C: SQLite (default if no PG vars set)
# HC_SQLITE_PATH=./data/hippocampus.sqlite

# ── Embedding Provider ─────────────────────
# Provider: native | openai | ollama
HC_EMBEDDER_PROVIDER=native

# Model (depends on provider):
#   - openai: text-embedding-3-small (default), text-embedding-3-large, text-embedding-ada-002
#   - ollama: nomic-embed-text (default), mxbai-embed-large, all-minilm
#   - native: ignored (hash-based, no model)
# HC_EMBEDDER_MODEL=nomic-embed-text

# Embedding dimensions:
#   - openai: 1536 (default for small), 3072 (for large)
#   - ollama: 768 (default for nomic), varies by model
#   - native: 384 (default)
# HC_EMBEDDING_DIMENSIONS=768

# ── Completion Provider ────────────────────
# Provider: native | openai | ollama
HC_COMPLETION_PROVIDER=native

# Model (depends on provider):
#   - openai: gpt-4-turbo (default), gpt-4o, gpt-3.5-turbo
#   - ollama: llama3 (default), llama3.2, mistral, phi3
#   - native: ignored (passthrough, no extraction)
# HC_COMPLETION_MODEL=llama3.2

# ── OpenAI (only if using openai provider) ─
# HC_OPENAI_API_KEY=sk-...

# ── Ollama (only if using ollama provider) ─
# HC_OLLAMA_BASE_URL=http://localhost:11434

# ── Custom Prompts ────────────────────────
# Must instruct LLM to return JSON with: facts[], strand, temporalFacts[]
# HC_EXTRACTION_PROMPT=Your custom extraction prompt here...
